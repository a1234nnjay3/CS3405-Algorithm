{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAME: 張子健         Student ID: 103011230\n",
    "### CS3405 Homework 3         \n",
    "#### Deep Learning Neural Networks\n",
    "#### Due: Monday, November 19, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUMMARY\n",
    "## MLP:\n",
    "### Trainning Accuracy reaches 0.9804 using learning rate= 0.1, batch size=30, activation function=Sigmoid, number of epoch= 500.\n",
    "* Learning rate=0.1 is a optimal value for [784,50,10] MLP. If learning rate becomes too low, accuracy dramatically decreases. Same phenomenon happens when learning is too high, but different MLP model can have different optimal learning rate.\n",
    "* Batch Size has significant impact on the trainning time, and it can also influence the trainning accuracy, so we need to find the optimal value with acceptable trainning time and accuracy. In my case, batch size=30 is a good value for MNIST-60000 dataset.\n",
    "* For activation function, I have tried ReLU, LeakyReLU and sigmoid. Overall, in this MLP model, sigmoid performs better at the result(accuracy).\n",
    "* Number of epoch will definitely contribute to the trainning accuracy, but it will rach a bottleneck where the accuracy stop increasing.(Other hyperparameters are the factors determining the limit) Since I only have personal laptop and limited time, I choose a set of hyperparameters and trainned it for around 2 hours with my i5-CPU.\n",
    "## CNN:\n",
    "### Trainning Accuracy reaches 0.9062 using learning rate= 0.001, batch size=200, activation function=ReLU, number of epoch= 4.\n",
    "* Learning rate for CNN should be lower than MLP, so I uesd 0.001. If learning rate becomes too low, accuracy dramatically decreases. Same phenomenon happens when learning is too high, but different MLP model can have different optimal learning rate.\n",
    "* Batch Size has significant impact on the trainning time, and it can also influence the trainning accuracy, so we need to find the optimal value with acceptable trainning time and accuracy. In my case, batch size=500 have been tried but with bad accuracy, so I finally used batch size=200.\n",
    "* For activation function, I have used ReLU, from the suggestion given by experts on StakeOverFlow.\n",
    "* Number of epoch will definitely contribute to the trainning accuracy, but it will rach a bottleneck where the accuracy stop increasing.(Other hyperparameters are the factors determining the limit) Since I only have personal laptop and limited time, I choose a set of hyperparameters and trainned it overnight with my i5-CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    def __init__(self, layerlist):\n",
    "        self.numLayers = len(layerlist)\n",
    "        self.layers = [{} for i in range(self.numLayers)]\n",
    "        self.lossList = []\n",
    "        #np.random.randn : Build a matrix with E(X)=0 and standard deviation=1\n",
    "        for i in range(1, self.numLayers):\n",
    "            self.layers[i][\"W\"] = np.random.randn(layerlist[i], layerlist[i-1])\n",
    "            self.layers[i][\"b\"] = np.random.randn(layerlist[i], 1)\n",
    "        return\n",
    "            \n",
    "    def forward(self, x, bs):\n",
    "        #copy x(input) to self.layers[0][\"a\"] without modifying x\n",
    "        self.layers[0][\"a\"] = np.copy(x)\n",
    "        for i in range(1, self.numLayers):\n",
    "            self.layers[i][\"z\"] = self.layers[i][\"W\"].dot(self.layers[i-1][\"a\"]) + self.layers[i][\"b\"]\n",
    "            self.layers[i][\"a\"] = self.activation(self.layers[i][\"z\"])\n",
    "        self.p = self.softmax(self.layers[-1][\"a\"])\n",
    "        self.y = np.zeros(self.p.shape, dtype=int)\n",
    "        #\n",
    "        for j, i in enumerate(self.p.argmax(axis=0)):\n",
    "            self.y[i, j] = 1\n",
    "        return\n",
    "    \n",
    "    #Sigmoid:  f(x) = 1 / 1 + exp(-x)\n",
    "    def activation(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    #Hyperbolic Tangent function:  f(x) = 1 — exp(-2x) / 1 + exp(-2x)\n",
    "    def tanh(self, z):\n",
    "        return (1-np.exp(-2*z))/(1+np.exp(-2*z))\n",
    "    \n",
    "    def ReLU(self, z):\n",
    "        a = np.copy(z)\n",
    "        a[a<0] = 0.0\n",
    "        return a\n",
    "    \n",
    "    #Leaky ReLU\n",
    "    def PReLU(self, z):\n",
    "        return z if z>0 else z/10\n",
    "    \n",
    "    #dadz = inverseActivation\n",
    "    def inverseActivation(self, a):\n",
    "        return a * (1.0 - a)\n",
    "    \n",
    "    def loss(self, y): \n",
    "        return np.mean(- np.log(self.p) * y - np.log(1.0 - self.p) * (1.0 - y) )\n",
    "    \n",
    "    # P=softmax(a)\n",
    "    def softmax(self, a):\n",
    "        expa = np.exp(a)\n",
    "        return (expa / np.sum(expa, axis = 0))\n",
    "    \n",
    "    def backprop(self, y, bs):\n",
    "        self.dJdp = - y / self.p + (1.0 - y) / (1.0 - self.p)\n",
    "        #a 3-dimensional arry\n",
    "        dpda = np.zeros((self.p.shape[0], self.p.shape[0], bs))\n",
    "        for b in range(bs):\n",
    "            for i in range(dpda.shape[0]):\n",
    "                for j in range(dpda.shape[1]):\n",
    "                    dpda[i, j, b] = (self.p[i, b] - self.p[i, b] ** 2) if i == j else - self.p[i, b] * self.p[j, b]\n",
    "        \n",
    "        self.layers[-1][\"dJda\"] = np.zeros(self.layers[-1][\"a\"].shape)\n",
    "        for b in range(bs):\n",
    "            self.layers[-1][\"dJda\"][:, b] = dpda[:, :, b].dot(self.dJdp[:, b])\n",
    "          \n",
    "        for i in range(self.numLayers - 1, 0, -1):\n",
    "            self.layers[i][\"dJdz\"] = self.inverseActivation(self.layers[i][\"a\"]) * self.layers[i][\"dJda\"]\n",
    "            self.layers[i][\"dJdb\"] = np.mean(self.layers[i][\"dJdz\"], axis = 1).reshape(self.layers[i][\"b\"].shape) #?\n",
    "            self.layers[i][\"dJdW\"] = self.layers[i][\"dJdz\"].dot(self.layers[i-1][\"a\"].T) / bs #?\n",
    "            self.layers[i-1][\"dJda\"] = self.layers[i][\"W\"].T.dot(self.layers[i][\"dJdz\"])\n",
    "        return\n",
    "            \n",
    "    def update(self, lr = 0.01):\n",
    "        for i in range(1, self.numLayers):\n",
    "            self.layers[i][\"W\"] -= lr * self.layers[i][\"dJdW\"]\n",
    "            self.layers[i][\"b\"] -= lr * self.layers[i][\"dJdb\"]\n",
    "        return\n",
    "    #a, b = 1-dimensional array\n",
    "    def shuffle(self, a, b):\n",
    "        shuffled_a = np.copy(a)\n",
    "        shuffled_b = np.copy(b)\n",
    "        permutation = np.random.permutation(a.shape[1])\n",
    "        #permutation = a number with size equal to the length of a. e.g. batch size\n",
    "        for oldindex, newindex in enumerate(permutation):\n",
    "            shuffled_a[:, oldindex] = a[:, newindex]\n",
    "            shuffled_b[:, oldindex] = b[:, newindex]\n",
    "        return shuffled_a, shuffled_b\n",
    "        \n",
    "    def train(self, trainX, trainY, numEpoch=1, lr=0.01, bs=2):  \n",
    "        for e in range(numEpoch):\n",
    "            shuffled_trainX, shuffled_trainY = self.shuffle(trainX, trainY)\n",
    "            for i in range(trainX.shape[1] // bs):\n",
    "                #冒號：指定範圍, from ith batch to (i+1) batch\n",
    "                x = shuffled_trainX[:, i*bs : (i+1)*bs]\n",
    "                y = shuffled_trainY[:, i*bs : (i+1)*bs]\n",
    "                self.forward(x, bs)\n",
    "                self.lossList.append(self.loss(y))  #?\n",
    "                self.backprop(y, bs)\n",
    "                self.update(lr)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv2xy(fileName):\n",
    "    f = open(fileName, \"r\")\n",
    "    a = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for line in a:\n",
    "        linepixels = [int(pixel) for pixel in line.split(\",\")]\n",
    "        x.append(linepixels[1:])\n",
    "        y.append(linepixels[0])\n",
    "    \n",
    "    \n",
    "    out_x = np.clip(np.array(x).T, 0, 1)\n",
    "    out_y = np.zeros((10, len(y)), dtype=int)\n",
    "    for i in range(len(y)):\n",
    "        out_y[y[i], i] = 1\n",
    "        \n",
    "    return out_x, out_y\n",
    "    \n",
    "train_x_60000, train_y_60000 = csv2xy(\"mnist_train_60000.csv\")\n",
    "test_x_10000, test_y_10000 = csv2xy(\"mnist_test_10000.csv\")\n",
    "train_x_100, train_y_100 = csv2xy(\"mnist_train_100.csv\")\n",
    "test_x_10, test_y_10 = csv2xy(\"mnist_test_10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnMNIST60000 = MLP([784, 50, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15039d9b0>]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nnMNIST60000.train(train_x_60000, train_y_60000, numEpoch=100, lr=0.1, bs = 30)\n",
    "plt.plot(nnMNIST60000.lossList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy= 0.9804333333333334\n"
     ]
    }
   ],
   "source": [
    "nnMNIST60000.forward(train_x_60000, 60000)\n",
    "predicted = nnMNIST60000.y.argmax(axis=0)\n",
    "answer = train_y_60000.argmax(axis=0)\n",
    "accuracy = 1.0 - float(len(np.nonzero(predicted - answer)[0])) / len(answer)\n",
    "print (\"Training Accuracy=\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://upload-images.jianshu.io/upload_images/1817489-8a396b10074d0750.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://upload-images.jianshu.io/upload_images/1817489-8a396b10074d0750.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glorot_uniform(shape, num_neurons_in, num_neurons_out):\n",
    "    scale = np.sqrt(6. / (num_neurons_in + num_neurons_out))\n",
    "    return np.random.uniform(low=-scale, high=scale, size=shape)\n",
    "\n",
    "def zero(shape, *args):\n",
    "    return np.zeros(shape)\n",
    "### activations ################################################################\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def der_sigmoid(x, y=None):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(x):\n",
    "    e = np.exp(x - np.max(x))\n",
    "    return e / np.sum(e)\n",
    "\n",
    "def der_softmax(x, y=None):\n",
    "    s = softmax(x)\n",
    "    if y is not None:\n",
    "        k = s[np.where(y == 1)]\n",
    "        a = - k * s\n",
    "        a[np.where(y == 1)] = k * (1 - k)\n",
    "        return a\n",
    "    return s * (1 - s)\n",
    "\n",
    "    def ReLU(self, z):\n",
    "        a = np.copy(z)\n",
    "        a[a<0] = 0.0\n",
    "        return a\n",
    "    \n",
    "    def ReLUPrime(self, a):\n",
    "        dadz = np.copy(a)\n",
    "        dadz[a>0] = 1.0\n",
    "        return dadz\n",
    "    \n",
    "    def LeakyReLU(self, z, leakyrate):\n",
    "        a = np.copy(z)\n",
    "        (rows, cols) = a.shape\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                a[i, j] = z[i, j] if z[i,j] > 0 else (leakyrate * z[i, j])\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(metaclass=abc.ABCMeta):\n",
    "    def __init__(self):\n",
    "        self.depth = None\n",
    "        self.height = None\n",
    "        self.width = None\n",
    "        self.n_out = None\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def connect_to(self, prev_layer):\n",
    "        raise AssertionError\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def feedforward(self, prev_layer):\n",
    "        raise AssertionError\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def backpropagate(self, prev_layer, delta):\n",
    "        raise AssertionError\n",
    "\n",
    "\n",
    "class InputLayer(Layer):\n",
    "    def __init__(self, height, width):\n",
    "        super().__init__()\n",
    "        self.depth = 1\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.n_out = self.depth * self.height * self.width\n",
    "        self.der_act_func = lambda x: x\n",
    "\n",
    "    def connect_to(self, prev_layer):\n",
    "        raise AssertionError\n",
    "\n",
    "    def feedforward(self, prev_layer):\n",
    "        raise AssertionError\n",
    "\n",
    "    def backpropagate(self, prev_layer, delta):\n",
    "        raise AssertionError\n",
    "\n",
    "\n",
    "class FullyConnectedLayer(Layer):\n",
    "    def __init__(self, height, init_func, act_func):\n",
    "        super().__init__()\n",
    "        self.depth = 1\n",
    "        self.height = height\n",
    "        self.width = 1\n",
    "        self.n_out = self.depth * self.height * self.width\n",
    "        self.init_func = init_func\n",
    "        self.act_func = act_func\n",
    "        self.der_act_func = getattr(f, \"der_%s\" % act_func.__name__)\n",
    "\n",
    "    def connect_to(self, prev_layer):\n",
    "        self.w = self.init_func((self.n_out, prev_layer.n_out), prev_layer.n_out, self.n_out)\n",
    "        self.b = f.zero((self.n_out, 1))\n",
    "\n",
    "    def feedforward(self, prev_layer):\n",
    "        \"\"\"\n",
    "        Feedforward the observation through the layer\n",
    "\n",
    "        :param prev_layer: the previous layer of the network\n",
    "        \"\"\"\n",
    "        prev_a = prev_layer.a.reshape((prev_layer.a.size, 1))\n",
    "\n",
    "        self.z = (self.w @ prev_a) + self.b\n",
    "\n",
    "        self.a = self.act_func(self.z)\n",
    "        assert self.z.shape == self.a.shape\n",
    "\n",
    "    def backpropagate(self, prev_layer, delta):\n",
    "        \"\"\"\n",
    "        Backpropagate the error through the layer\n",
    "\n",
    "        :param prev_layer: the previous layer of the network\n",
    "        :param delta: the error propagated backward by the next layer of the network\n",
    "        :returns: the amount of change of input weights of this layer, the amount of change of the biases of this layer\n",
    "            and the error propagated by this layer\n",
    "        \"\"\"\n",
    "        assert delta.shape == self.z.shape == self.a.shape\n",
    "\n",
    "        prev_a = prev_layer.a.reshape((prev_layer.a.size, 1))\n",
    "\n",
    "        der_w = delta @ prev_a.T\n",
    "\n",
    "        der_b = np.copy(delta)\n",
    "\n",
    "        prev_delta = (self.w.T @ delta).reshape(prev_layer.z.shape) * prev_layer.der_act_func(prev_layer.z)\n",
    "\n",
    "        return der_w, der_b, prev_delta\n",
    "\n",
    "\n",
    "class ConvolutionalLayer(Layer):\n",
    "    def __init__(self, depth, kernel_size, init_func, act_func):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.kernel_size = kernel_size\n",
    "        self.init_func = init_func\n",
    "        self.act_func = act_func\n",
    "        self.der_act_func = getattr(f, \"der_%s\" % act_func.__name__)\n",
    "\n",
    "    def connect_to(self, prev_layer):\n",
    "        self.stride_length = 1\n",
    "        self.height = ((prev_layer.height - self.kernel_size) // self.stride_length) + 1\n",
    "        self.width  = ((prev_layer.width  - self.kernel_size) // self.stride_length) + 1\n",
    "        self.n_out = self.depth * self.height * self.width\n",
    "\n",
    "        self.w = self.init_func((self.depth, prev_layer.depth, self.kernel_size, self.kernel_size),\n",
    "            prev_layer.n_out, self.n_out)\n",
    "        self.b = f.zero((self.depth, 1))\n",
    "\n",
    "    def feedforward(self, prev_layer):\n",
    "        \"\"\"\n",
    "        Feedforward the observation through the layer\n",
    "\n",
    "        :param prev_layer: the previous layer of the network. The activations of the previous layer must be a list of\n",
    "            feature maps, where each feature map is a 2d matrix\n",
    "        \"\"\"\n",
    "        assert self.w.shape == (self.depth, prev_layer.depth, self.kernel_size, self.kernel_size)\n",
    "        assert self.b.shape == (self.depth, 1)\n",
    "        assert prev_layer.a.ndim == 3\n",
    "\n",
    "        prev_a = prev_layer.a\n",
    "\n",
    "        filters_c_out = self.w.shape[0]\n",
    "        filters_c_in = self.w.shape[1]\n",
    "        filters_h = self.w.shape[2]\n",
    "        filters_w = self.w.shape[3]\n",
    "\n",
    "        image_c = prev_a.shape[0]\n",
    "        assert image_c == filters_c_in\n",
    "        image_h = prev_a.shape[1]\n",
    "        image_w = prev_a.shape[2]\n",
    "\n",
    "        stride = 1\n",
    "        new_h = ((image_h - filters_h) // stride) + 1\n",
    "        new_w = ((image_w - filters_w) // stride) + 1\n",
    "\n",
    "        self.z = np.zeros((filters_c_out, new_h, new_w))\n",
    "        for r in range(filters_c_out):\n",
    "            for t in range(image_c):\n",
    "                filter = self.w[r, t]\n",
    "                for i, m in enumerate(range(0, image_h - filters_h + 1, self.stride_length)):\n",
    "                    for j, n in enumerate(range(0, image_w - filters_w + 1, self.stride_length)):\n",
    "                        prev_a_window = prev_a[t, m:m+filters_h, n:n+filters_w]\n",
    "                        self.z[r, i, j] += np.correlate(prev_a_window.ravel(), filter.ravel(), mode=\"valid\")\n",
    "\n",
    "        for r in range(self.depth):\n",
    "            self.z[r] += self.b[r]\n",
    "\n",
    "        self.a = np.vectorize(self.act_func)(self.z)\n",
    "        assert self.a.shape == self.z.shape\n",
    "\n",
    "    def backpropagate(self, prev_layer, delta):\n",
    "        \"\"\"\n",
    "        Backpropagate the error through the layer\n",
    "\n",
    "        :param prev_layer: the previous layer of the network. The activations of the previous layer are a list of\n",
    "            feature maps, where each feature map is a 2d matrix\n",
    "        :param delta:\n",
    "        \"\"\"\n",
    "        assert delta.shape[0] == self.depth\n",
    "\n",
    "        prev_a = prev_layer.a\n",
    "\n",
    "        der_w = np.empty_like(self.w)\n",
    "        for r in range(self.depth):\n",
    "            for t in range(prev_layer.depth):\n",
    "                for h in range(self.kernel_size):\n",
    "                    for v in range(self.kernel_size):\n",
    "                        prev_a_window = prev_a[t, v:v+self.height-self.kernel_size+1:self.stride_length,\n",
    "                                                  h:h+self.width -self.kernel_size+1:self.stride_length]\n",
    "                        delta_window  =  delta[r, v:v+self.height-self.kernel_size+1:self.stride_length,\n",
    "                                                  h:h+self.width -self.kernel_size+1:self.stride_length]\n",
    "                        assert prev_a_window.shape == delta_window.shape\n",
    "                        der_w[r, t, h, v] = np.sum(prev_a_window * delta_window)\n",
    "\n",
    "        der_b = np.empty((self.depth, 1))\n",
    "        for r in range(self.depth):\n",
    "            der_b[r] = np.sum(delta[r])\n",
    "\n",
    "        prev_delta = np.zeros_like(prev_a)\n",
    "        for r in range(self.depth):\n",
    "            for t in range(prev_layer.depth):\n",
    "                kernel = self.w[r, t]\n",
    "                for i, m in enumerate(range(0, prev_layer.height - self.kernel_size + 1, self.stride_length)):\n",
    "                    for j, n in enumerate(range(0, prev_layer.width - self.kernel_size + 1, self.stride_length)):\n",
    "                        prev_delta[t, m:m+self.kernel_size, n:n+self.kernel_size] += kernel * delta[r, i, j]\n",
    "        prev_delta *= prev_layer.der_act_func(prev_layer.z)\n",
    "\n",
    "        return der_w, der_b, prev_delta\n",
    "\n",
    "\n",
    "class MaxPoolingLayer(Layer):\n",
    "    def __init__(self, pool_size):\n",
    "        super().__init__()\n",
    "        self.pool_size = pool_size\n",
    "        self.der_act_func = lambda x: x\n",
    "\n",
    "    def connect_to(self, prev_layer):\n",
    "        assert isinstance(prev_layer, ConvolutionalLayer)\n",
    "        self.depth = prev_layer.depth\n",
    "        self.height = ((prev_layer.height - self.pool_size) // self.pool_size) + 1\n",
    "        self.width  = ((prev_layer.width  - self.pool_size) // self.pool_size) + 1\n",
    "        self.n_out = self.depth * self.height * self.width\n",
    "\n",
    "        self.w = np.empty((0))\n",
    "        self.b = np.empty((0))\n",
    "\n",
    "    def feedforward(self, prev_layer):\n",
    "        \"\"\"\n",
    "        Feedforward the observation through the layer\n",
    "\n",
    "        :param prev_layer: the previous layer of the network. The activations of the previous layer must be a list of\n",
    "            feature maps, where each feature map is a 3d matrix\n",
    "        \"\"\"\n",
    "        assert self.w.size == 0\n",
    "        assert self.b.size == 0\n",
    "        assert isinstance(prev_layer, ConvolutionalLayer)\n",
    "        assert prev_layer.depth == self.depth\n",
    "        assert prev_layer.a.ndim == 3\n",
    "\n",
    "        prev_a = prev_layer.a\n",
    "\n",
    "        prev_layer_fmap_size = prev_layer.height\n",
    "        assert prev_layer_fmap_size % self.pool_size == 0\n",
    "\n",
    "        self.z = np.zeros((self.depth, self.height, self.width))\n",
    "        for r, t in zip(range(self.depth), range(prev_layer.depth)):\n",
    "            assert r == t\n",
    "            for i, m in enumerate(range(0, prev_layer.height, self.pool_size)):\n",
    "                for j, n in enumerate(range(0, prev_layer.width, self.pool_size)):\n",
    "                    prev_a_window = prev_a[t, m:m+self.pool_size, n:n+self.pool_size]\n",
    "                    assert prev_a_window.shape == (self.pool_size, self.pool_size)\n",
    "                    # downsampling\n",
    "                    self.z[r, i, j] = np.max(prev_a_window)\n",
    "\n",
    "        self.a = self.z\n",
    "\n",
    "    def backpropagate(self, prev_layer, delta):\n",
    "        \"\"\"\n",
    "        Backpropagate the error through the layer. Given any pair source(convolutional)/destination(pooling) feature\n",
    "        maps, each unit of the destination feature map propagates an error to a window (self.pool_size, self.pool_size)\n",
    "        of the source feature map\n",
    "\n",
    "        :param prev_layer: the previous layer of the network\n",
    "        :param delta: a tensor of shape (self.depth, self.height, self.width)\n",
    "        \"\"\"\n",
    "        assert self.w.size == 0\n",
    "        assert self.b.size == 0\n",
    "        assert isinstance(prev_layer, ConvolutionalLayer)\n",
    "        assert prev_layer.depth == self.depth\n",
    "        assert prev_layer.a.ndim == 3\n",
    "        assert delta.shape == (self.depth, self.height, self.width)\n",
    "\n",
    "        prev_a = prev_layer.a\n",
    "\n",
    "        der_w = np.array([])\n",
    "\n",
    "        der_b = np.array([])\n",
    "\n",
    "        prev_delta = np.empty_like(prev_a)\n",
    "        for r, t in zip(range(self.depth), range(prev_layer.depth)):\n",
    "            assert r == t\n",
    "            for i, m in enumerate(range(0, prev_layer.height, self.pool_size)):\n",
    "                for j, n in enumerate(range(0, prev_layer.width, self.pool_size)):\n",
    "                    prev_a_window = prev_a[t, m:m+self.pool_size, n:n+self.pool_size]\n",
    "                    assert prev_a_window.shape == (self.pool_size, self.pool_size)\n",
    "                    # upsampling: the unit which was the max at the forward propagation\n",
    "                    # receives all the error at backward propagation (the other units receive zero)\n",
    "                    max_unit_index = np.unravel_index(prev_a_window.argmax(), prev_a_window.shape)\n",
    "                    prev_delta_window = np.zeros_like(prev_a_window)\n",
    "                    prev_delta_window[max_unit_index] = delta[t, i, j]\n",
    "                    prev_delta[r, m:m+self.pool_size, n:n+self.pool_size] = prev_delta_window\n",
    "\n",
    "        return der_w, der_b, prev_delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.41657829285, Accuracy: 0.73125\n",
      "Epoch: 2, Loss: 0.419774413109, Accuracy: 0.875\n",
      "Epoch: 3, Loss: 0.313294112682, Accuracy: 0.890625\n",
      "Epoch: 4, Loss: 0.285170167685, Accuracy: 0.90625\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, layers, loss_func):\n",
    "        assert len(layers) > 0\n",
    "\n",
    "        assert isinstance(layers[0], l.InputLayer)\n",
    "        self.input_layer = layers[0]\n",
    "\n",
    "        assert isinstance(layers[-1], l.FullyConnectedLayer)\n",
    "        self.output_layer = layers[-1]\n",
    "\n",
    "        self.layers = [(prev_layer, layer) for prev_layer, layer in zip(layers[:-1], layers[1:])]\n",
    "\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "        for prev_layer, layer in self.layers:\n",
    "            layer.connect_to(prev_layer)\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        self.input_layer.z = x\n",
    "        self.input_layer.a = x\n",
    "\n",
    "        for prev_layer, layer in self.layers:\n",
    "            layer.feedforward(prev_layer)\n",
    "\n",
    "    def backpropagate(self, batch, optimizer):\n",
    "        sum_der_w = {layer: np.zeros_like(layer.w) for _, layer in self.layers}\n",
    "        sum_der_b = {layer: np.zeros_like(layer.b) for _, layer in self.layers}\n",
    "\n",
    "        for x, y in batch:\n",
    "            self.feedforward(x)\n",
    "\n",
    "            # propagate the error backward\n",
    "            loss = self.loss_func(self.output_layer.a, y)\n",
    "            delta = loss * self.output_layer.der_act_func(self.output_layer.z, y)\n",
    "            for prev_layer, layer in reversed(self.layers):\n",
    "                der_w, der_b, prev_delta = layer.backpropagate(prev_layer, delta)\n",
    "                sum_der_w[layer] += der_w\n",
    "                sum_der_b[layer] += der_b\n",
    "                delta = prev_delta\n",
    "\n",
    "        # update weights and biases\n",
    "        optimizer.apply(self.layers, sum_der_w, sum_der_b, len(batch))\n",
    "\n",
    "\n",
    "def train(net, optimizer, num_epochs, batch_size, trn_set, vld_set=None):\n",
    "    assert isinstance(net, NeuralNetwork)\n",
    "    assert num_epochs > 0\n",
    "    assert batch_size > 0\n",
    "\n",
    "    trn_x, trn_y = trn_set\n",
    "    inputs = [(x, y) for x, y in zip(trn_x, trn_y)]\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        np.random.shuffle(inputs)\n",
    "\n",
    "        # divide input observations into batches\n",
    "        batches = [inputs[j:j+batch_size] for j in range(0, len(inputs), batch_size)]\n",
    "        inputs_done = 0\n",
    "    for i in range(total_iters):\n",
    "        workspace.RunNet(train_model.net)\n",
    "        accuracy[i] = workspace.blobs['accuracy']\n",
    "        loss[i] = workspace.blobs['loss']\n",
    "        # Check the accuracy and loss every so often\n",
    "        if i % 25 == 0:\n",
    "            print(\"Epoch: {}, Loss: {}, Accuracy: {}\".format(i,loss[i],accuracy[i]))\n",
    "\n",
    "BatchSize = 200\n",
    "EpochCount = 4\n",
    "LearningRate = 0.001\n",
    "Eta = 0.001\n",
    "\n",
    "input_data_spec = [BatchSize, 1, 28, 28] \n",
    "conv_layer_spec = [{\"k_num\" : 6, \"k_h\" : 5, \"k_w\" : 5, \"stride\" : 1, \"zp\" : 0, \"mph\" : 2,\"mpw\" :2},\n",
    "                   {\"k_num\" : 16, \"k_h\" : 3, \"k_w\" : 3, \"stride\" : 1, \"zp\" : 0, \"mph\" : 2,\"mpw\" :2}]\n",
    "fc_layer_spec = [400,120,84,10]\n",
    "\n",
    "cnn60000 = CNN(input_data_spec=input_data_spec, \n",
    "          conv_layer_spec=conv_layer_spec, \n",
    "          fc_layer_spec=fc_layer_spec)\n",
    "\n",
    "cnn60000.train(train_60000_x, train_60000_y,train_60000_x, train_60000_y, EpochCount, LearningRate, Eta)\n",
    "cnn60000.train(train_60000_x, train_60000_y, EpochCount, LearningRate, Eta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
